{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "authorship_tag": "ABX9TyMh5G4KF+uLK67ylHElbTkA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredqbit/datasciencecoursera/blob/master/sepulvedaADDS-8515-5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 5: Canonical Correlation Analysis and Multivariate Regression on the Linnerud Dataset\n",
        "\n",
        "This notebook implements Canonical Correlation Analysis (CCA) and Multivariate Multiple Regression (MVR)\n",
        "on the Linnerud exercise dataset, following the methodology described in the report."
      ],
      "metadata": {
        "id": "KDc_-7S7MeuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import load_linnerud\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cross_decomposition import CCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from scipy.stats import f, ncf\n",
        "\n",
        "%pip install statsmodels --quiet\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.multivariate.manova import MANOVA\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "FIG_DIR = \"figures\"\n",
        "os.makedirs(FIG_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "KP8TxdDTMhO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions: correlation matrix, VIFs, and canonical loadings and redundancy"
      ],
      "metadata": {
        "id": "_Lr1qrFZND5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def col_corr_matrix(A, B):\n",
        "    \"\"\"Correlation matrix between columns of A and B.\"\"\"\n",
        "    A0 = A - A.mean(axis=0)\n",
        "    B0 = B - B.mean(axis=0)\n",
        "    cov = A0.T @ B0 / (A.shape[0] - 1)\n",
        "    sdA = A0.std(axis=0, ddof=1)\n",
        "    sdB = B0.std(axis=0, ddof=1)\n",
        "    return cov / np.outer(sdA, sdB)\n",
        "\n",
        "def redundancy_indices(loadings, rho_sq):\n",
        "    \"\"\"\n",
        "    Redundancy indices for one set given loadings and squared canonical corrs.\n",
        "    loadings: (variables x components) correlations with canonical variates\n",
        "    rho_sq : array of squared canonical correlations\n",
        "    \"\"\"\n",
        "    var_explained = (loadings ** 2).mean(axis=0)\n",
        "    return var_explained * rho_sq\n",
        "\n",
        "def compute_vif(X_df):\n",
        "    vifs = {}\n",
        "    for col in X_df.columns:\n",
        "        X_other = X_df.drop(columns=[col])\n",
        "        y = X_df[col]\n",
        "        model = LinearRegression().fit(X_other, y)\n",
        "        r2 = model.score(X_other, y)\n",
        "        vif = 1.0 / (1.0 - r2)\n",
        "        vifs[col] = vif\n",
        "    return pd.Series(vifs, name=\"VIF\")\n",
        "\n",
        "def calculate_manova_sample_size(effect_size_f, alpha, power, num_groups, num_dependent_vars, max_n=1000):\n",
        "    \"\"\"\n",
        "    Calculates the minimum total sample size for a one-way MANOVA.\n",
        "\n",
        "    Parameters:\n",
        "    effect_size_f (float): The effect size 'f' (not f-squared, common in G*Power input for f)\n",
        "    alpha (float): Significance level (e.g., 0.05)\n",
        "    power (float): Desired power (e.g., 0.80)\n",
        "    num_groups (int): Number of independent variable groups\n",
        "    num_dependent_vars (int): Number of dependent variables\n",
        "    max_n (int): Maximum sample size to search up to\n",
        "    \"\"\"\n",
        "    if effect_size_f <= 0:\n",
        "        raise ValueError(\"Effect size f must be greater than 0\")\n",
        "\n",
        "    # G*Power uses 'f' as the effect size, which is sqrt(f^2) in some contexts.\n",
        "    # The noncentrality parameter (ncp) formula is typically n * s * eta^2_p\n",
        "    # where eta^2_p is the partial eta squared.\n",
        "    # G*Power maps f to ncp internally based on specific formula for MANOVA.\n",
        "    # We will use an iterative approach based on the F distribution's power function.\n",
        "\n",
        "    # The actual calculation requires mapping the effect size 'f' to a noncentrality parameter.\n",
        "    # For one-way MANOVA, the approach often uses the noncentral F-distribution.\n",
        "    # This example uses a simplified approach that would be accurate if 'f' is correctly defined for the power analysis type.\n",
        "    # A full G*Power replication is very complex, so this demonstrates the iterative concept with a placeholder ncp formula\n",
        "    # that would need adjustment based on the exact G*Power definition.\n",
        "\n",
        "    # The Real Statistics website provides the underlying logic using F distribution power\n",
        "    # G*Power uses 'f' effect size where f^2 = eta^2 / (1 - eta^2)\n",
        "    # The noncentrality parameter is roughly related to N * f^2 *... something complex for MANOVA.\n",
        "\n",
        "    # This is a conceptual implementation of the iterative search.\n",
        "    # For a *precise* answer, using G*Power or a specialized R package is recommended.\n",
        "    # The `statsmodels` package does not have a ready-made function for this.\n",
        "\n",
        "    for n_total in range(num_groups + 10, max_n + 1, 1): # Start with a plausible minimum\n",
        "        df1 = num_dependent_vars * (num_groups - 1)\n",
        "        df2 = num_dependent_vars * (n_total - num_groups)\n",
        "\n",
        "        # This part requires a correct formula for ncp based on effect size 'f' and specific test\n",
        "        # G*Power is very specific about this mapping\n",
        "        # Let's use a standard approximation for the noncentral F distribution as a demonstration\n",
        "        # Note: This is an approximation and might not match G*Power exactly without the precise formula.\n",
        "        ncp = (n_total - num_groups) * effect_size_f**2 # A general f^2 formula for some tests\n",
        "\n",
        "        # Calculate the critical F-value for the given alpha\n",
        "        f_crit = f.ppf(1 - alpha, df1, df2)\n",
        "\n",
        "        # Calculate the power for the current sample size using ncf.cdf for non-central F-distribution\n",
        "        calculated_power = 1 - ncf.cdf(f_crit, df1, df2, ncp)\n",
        "\n",
        "        if calculated_power >= power:\n",
        "            return n_total\n",
        "\n",
        "    return \"Max sample size reached, power not achieved. Try a larger max_n or a larger effect size.\""
      ],
      "metadata": {
        "id": "H4kku5ZbNH1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset and basic EDA (Step 1)"
      ],
      "metadata": {
        "id": "iex5r6sIMtUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linnerud = load_linnerud()\n",
        "\n",
        "X_ex = pd.DataFrame(linnerud.data, columns=linnerud.feature_names)        # Chins, Situps, Jumps\n",
        "Y_phys = pd.DataFrame(linnerud.target, columns=linnerud.target_names)     # Weight, Waist, Pulse\n",
        "\n",
        "print(\"Exercise variables (X):\")\n",
        "display(X_ex.head())\n",
        "\n",
        "print(\"\\nPhysiological variables (Y):\")\n",
        "display(Y_phys.head())\n",
        "\n",
        "print(\"\\nX info:\")\n",
        "print(X_ex.info())\n",
        "\n",
        "print(\"\\nY info:\")\n",
        "print(Y_phys.info())\n",
        "\n",
        "print(\"\\nSummary statistics for X:\")\n",
        "display(X_ex.describe())\n",
        "\n",
        "print(\"\\nSummary statistics for Y:\")\n",
        "display(Y_phys.describe())\n",
        "\n",
        "print(\"\\nMissing values in X:\")\n",
        "print(X_ex.isna().sum())\n",
        "print(\"\\nMissing values in Y:\")\n",
        "print(Y_phys.isna().sum())\n"
      ],
      "metadata": {
        "id": "4UYCD_rDMrz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation heatmap and simple outlier check"
      ],
      "metadata": {
        "id": "hYzyeGqXMz_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine for correlation inspection\n",
        "df_all = pd.concat([X_ex, Y_phys], axis=1)\n",
        "corr = df_all.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\",\n",
        "            annot_kws={\"size\": 9}, cbar_kws={\"label\": \"Correlation\"})\n",
        "plt.title(\"Correlation Matrix: Exercise and Physiological Variables\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIG_DIR, \"linnerud_correlation_matrix.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "z_scores = (df_all - df_all.mean()) / df_all.std(ddof=1)\n",
        "outlier_mask = (z_scores.abs() > 3)\n",
        "print(\"Potential outliers (|z| > 3) per variable:\")\n",
        "print(outlier_mask.sum())"
      ],
      "metadata": {
        "id": "Rs56BUcxM35P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization (Step 2)"
      ],
      "metadata": {
        "id": "qBp_2QIsM7Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize each block separately\n",
        "scaler_X = StandardScaler()\n",
        "scaler_Y = StandardScaler()\n",
        "\n",
        "X_scaled = scaler_X.fit_transform(X_ex)\n",
        "Y_scaled = scaler_Y.fit_transform(Y_phys)\n",
        "\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X_ex.columns)\n",
        "Y_scaled_df = pd.DataFrame(Y_scaled, columns=Y_phys.columns)\n",
        "\n",
        "X_scaled_df.head(), Y_scaled_df.head()"
      ],
      "metadata": {
        "id": "fm4j2RmYNAYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Canonical Correlation Analysis (Step 2 CCA)\n",
        "\n",
        "Note: CCA naturally takes two inputs (X, Y), so forcing it into a single sklearn Pipeline is awkward. We keep it as a clean, modular block using the standardized matrices."
      ],
      "metadata": {
        "id": "dl8T_QkJNLKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CCA with up to min(p, q) = 3 components\n",
        "cca = CCA(n_components=3)\n",
        "cca.fit(X_scaled, Y_scaled)\n",
        "\n",
        "X_c, Y_c = cca.transform(X_scaled, Y_scaled)  # canonical variates\n",
        "\n",
        "canonical_corrs = np.array([\n",
        "    np.corrcoef(X_c[:, k], Y_c[:, k])[0, 1]\n",
        "    for k in range(3)\n",
        "])\n",
        "\n",
        "print(\"Canonical correlations:\")\n",
        "for k, rho in enumerate(canonical_corrs, start=1):\n",
        "    print(f\"  rho_{k} = {rho:.3f}\")\n",
        "\n",
        "# Canonical loadings: correlations of original vars with canonical variates\n",
        "load_XU = col_corr_matrix(X_scaled, X_c)\n",
        "load_YV = col_corr_matrix(Y_scaled, Y_c)\n",
        "\n",
        "load_XU_df = pd.DataFrame(load_XU, index=X_ex.columns,\n",
        "                          columns=[f\"u{k}\" for k in range(1, 4)])\n",
        "load_YV_df = pd.DataFrame(load_YV, index=Y_phys.columns,\n",
        "                          columns=[f\"v{k}\" for k in range(1, 4)])\n",
        "\n",
        "print(\"\\nCanonical loadings: Exercise variables on U\")\n",
        "display(load_XU_df)\n",
        "\n",
        "print(\"\\nCanonical loadings: Physiological variables on V\")\n",
        "display(load_YV_df)\n",
        "\n",
        "rho_sq = canonical_corrs ** 2\n",
        "red_Y_given_X = redundancy_indices(load_YV, rho_sq)\n",
        "red_X_given_Y = redundancy_indices(load_XU, rho_sq)\n",
        "\n",
        "print(\"\\nRedundancy indices (Y | X):\", np.round(red_Y_given_X, 3))\n",
        "print(\"Sum redundancy Y | X:\", red_Y_given_X.sum())\n",
        "print(\"Redundancy indices (X | Y):\", np.round(red_X_given_Y, 3))\n",
        "print(\"Sum redundancy X | Y:\", red_X_given_Y.sum())"
      ],
      "metadata": {
        "id": "xswZiQSZNPE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CCA scatterplots (canonical variates)"
      ],
      "metadata": {
        "id": "H1oy61TLNT7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplot for first canonical pair\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X_c[:, 0], Y_c[:, 0])\n",
        "plt.axhline(0, color=\"gray\", linewidth=0.8)\n",
        "plt.axvline(0, color=\"gray\", linewidth=0.8)\n",
        "plt.xlabel(\"u1 (exercise canonical variate)\")\n",
        "plt.ylabel(\"v1 (physiological canonical variate)\")\n",
        "plt.title(\"CCA: First Canonical Variates (u1 vs v1)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIG_DIR, \"cca_u1_v1_scatter.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X_c[:, 1], Y_c[:, 1])\n",
        "plt.axhline(0, color=\"gray\", linewidth=0.8)\n",
        "plt.axvline(0, color=\"gray\", linewidth=0.8)\n",
        "plt.xlabel(\"u2 (exercise canonical variate)\")\n",
        "plt.ylabel(\"v2 (physiological canonical variate)\")\n",
        "plt.title(\"CCA: Second Canonical Variates (u2 vs v2)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIG_DIR, \"cca_u2_v2_scatter.png\"), dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OHThIkkdNXL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIF for predictors"
      ],
      "metadata": {
        "id": "V_e6qsbXNu4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vif_series = compute_vif(X_ex)\n",
        "print(\"Variance Inflation Factors (VIF):\")\n",
        "display(vif_series)"
      ],
      "metadata": {
        "id": "ksDOPRagNzLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimized pipeline for Multivariate Regression: fit and metrics (Step 3 MVR)"
      ],
      "metadata": {
        "id": "o9bOUqGONaOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pipeline: Standardize X -> LinearRegression (multi-output)\n",
        "mvr_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),       # scales predictors X_ex\n",
        "    (\"reg\",    LinearRegression())\n",
        "])\n",
        "\n",
        "mvr_pipeline.fit(X_ex, Y_phys)\n",
        "\n",
        "# Predictions\n",
        "Y_hat = pd.DataFrame(mvr_pipeline.predict(X_ex), columns=Y_phys.columns)\n",
        "\n",
        "# Per-response R^2 and RMSE\n",
        "r2_each = r2_score(Y_phys, Y_hat, multioutput=\"raw_values\")\n",
        "mse_each = mean_squared_error(Y_phys, Y_hat, multioutput=\"raw_values\")\n",
        "rmse_each = np.sqrt(mse_each)\n",
        "\n",
        "perf = pd.DataFrame({\n",
        "    \"Response\": Y_phys.columns,\n",
        "    \"R2\": r2_each,\n",
        "    \"RMSE\": rmse_each\n",
        "})\n",
        "\n",
        "print(\"In-sample multivariate regression performance (pipeline):\")\n",
        "display(perf)"
      ],
      "metadata": {
        "id": "NYvZxsdPNeZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual diagnostics (normality, homoscedasticity)"
      ],
      "metadata": {
        "id": "j03bNcI_NnjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = Y_phys - Y_hat\n",
        "\n",
        "for col in Y_phys.columns:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # Residual vs fitted\n",
        "    axes[0].scatter(Y_hat[col], residuals[col])\n",
        "    axes[0].axhline(0, color=\"gray\", linewidth=0.8)\n",
        "    axes[0].set_xlabel(\"Fitted values\")\n",
        "    axes[0].set_ylabel(\"Residuals\")\n",
        "    axes[0].set_title(f\"{col}: Residuals vs Fitted\")\n",
        "\n",
        "    # Q-Q plot\n",
        "    sm.qqplot(residuals[col], line=\"45\", ax=axes[1])\n",
        "    axes[1].set_title(f\"{col}: Q-Q Plot\")\n",
        "\n",
        "    fig.suptitle(f\"Diagnostics for {col}\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(FIG_DIR, f\"diagnostics_{col.lower()}.png\"), dpi=300)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EEsVSkssNrR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MANOVA for multivariate significance"
      ],
      "metadata": {
        "id": "nNRf6qRMNhVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a single DataFrame for MANOVA\n",
        "df_reg = pd.concat([Y_phys, X_ex], axis=1)\n",
        "formula = \"Weight + Waist + Pulse ~ Chins + Situps + Jumps\"\n",
        "\n",
        "maov = MANOVA.from_formula(formula, data=df_reg)\n",
        "print(maov.mv_test())"
      ],
      "metadata": {
        "id": "rBl5ozb4NkI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8061d4b2"
      },
      "source": [
        "Extracting F-values from MANOVA Output\n",
        "\n",
        "To programmatically retrieve the F-values for each predictor (Chins, Situps, Jumps) from the `maov` object, we can convert the `maov.mv_test()` output into a pandas DataFrame. This allows for easy indexing and selection of the desired statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed6b4894"
      },
      "source": [
        "manova_results = maov.mv_test()\n",
        "\n",
        "predictor_f_values = {}\n",
        "predictors = ['Intercept','Chins', 'Situps', 'Jumps']\n",
        "\n",
        "\n",
        "\n",
        "for p in predictors:\n",
        "    # Based on the inspection above, we will need to adjust the access path.\n",
        "    # For now, let's try to find a DataFrame that contains 'F Value' by iterating over values if it's a dict.\n",
        "    res_obj = manova_results.results[p]\n",
        "\n",
        "    f_value = None\n",
        "\n",
        "    # Strategy: Look for the 'stat' dataframe directly, which is common in statsmodels\n",
        "    if isinstance(res_obj, dict) and 'stat' in res_obj:\n",
        "         # This is a common structure: {'stat': DataFrame, ...}\n",
        "         f_value = res_obj['stat']['F Value'].iloc[0]\n",
        "    elif hasattr(res_obj, 'mv_tables'):\n",
        "         # Standard MultivariateResultsWrapper object\n",
        "         f_value = res_obj.mv_tables[0]['F Value'].iloc[0]\n",
        "    elif isinstance(res_obj, dict):\n",
        "         # It's a dict but maybe keys are different. Let's look for any DF with 'F Value'\n",
        "         for key, val in res_obj.items():\n",
        "             if isinstance(val, pd.DataFrame) and 'F Value' in val.columns:\n",
        "                 f_value = val['F Value'].iloc[0]\n",
        "                 break\n",
        "\n",
        "    if f_value is not None:\n",
        "        predictor_f_values[p] = f_value\n",
        "    else:\n",
        "        print(f\"Could not find F Value for {p}\")\n",
        "\n",
        "predictor_f_values_series = pd.Series(predictor_f_values, name=\"F Value\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e5952e9"
      },
      "source": [
        "Calculate Cohen's f-squared effect size for each predictor\n",
        "\n",
        "From the MANOVA output:\n",
        "\n",
        "*   F-value (F)\n",
        "*   Numerator Degrees of Freedom\n",
        "*   Denominator Degrees of Freedom\n",
        "\n",
        "First, we calculate Partial Eta-Squared ($\\eta_p^2$) using the formula:\n",
        "\n",
        "$$\\eta_p^2 = \\frac{\\text{numer_dfs} \\times F}{(\\text{numer_dfs} \\times F) + \\text{denom_dfs}}$$\n",
        "\n",
        "Then, we convert $\\eta_p^2$ to Cohen's f-squared ($f^2$) using the formula:\n",
        "\n",
        "$$f^2 = \\frac{\\eta_p^2}{1 - \\eta_p^2}$$\n",
        "\n",
        "This Cohen's f-squared value is used in a power analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e95ede3"
      },
      "source": [
        "# MANOVA statistics for 'Situps' from the previous\n",
        "F_value_chins = predictor_f_values_series['Chins']\n",
        "F_value_situps = predictor_f_values_series['Situps']\n",
        "F_value_jumps = predictor_f_values_series['Jumps']\n",
        "F_value_intercept = predictor_f_values_series['Intercept']\n",
        "numer_dfs = 3.0\n",
        "denom_dfs = 14.0\n",
        "\n",
        "# Calculate Partial Eta-Squared (eta_p^2)\n",
        "situps_partial_eta_squared = (numer_dfs * F_value_situps) / (numer_dfs * F_value_situps + denom_dfs)\n",
        "chins_partial_eta_squared = (numer_dfs * F_value_chins) / (numer_dfs * F_value_chins + denom_dfs)\n",
        "jumps_partial_eta_squared = (numer_dfs * F_value_jumps) / (numer_dfs * F_value_jumps + denom_dfs)\n",
        "intercept_partial_eta_squared = (numer_dfs * F_value_intercept) / (numer_dfs * F_value_intercept + denom_dfs)\n",
        "\n",
        "# Calculate Cohen's f-squared\n",
        "situps_cohen_f_squared = situps_partial_eta_squared / (1 - situps_partial_eta_squared)\n",
        "chins_cohen_f_squared = chins_partial_eta_squared / (1- chins_partial_eta_squared)\n",
        "jumps_cohen_f_squared = jumps_partial_eta_squared / (1 - jumps_partial_eta_squared)\n",
        "intercept_cohen_f_squared = intercept_partial_eta_squared / (1 - intercept_partial_eta_squared)\n",
        "\n",
        "# Parameters for a one-way MANOVA power analysis:\n",
        "groups = 3 # e.g., no. predictors\n",
        "dependent_vars = 3 # no. responses\n",
        "\n",
        "# --- Optimization Start ---\n",
        "# Goal: Maximize Power [0.70, 0.80] and Minimize Alpha [0.05, 0.10] such that situps_min_n <= 20.\n",
        "\n",
        "possible_powers = np.linspace(0.80, 0.60, 200) # Search from high to low\n",
        "possible_alphas = np.linspace(0.05, 0.10, 200) # Search from low to high\n",
        "\n",
        "opt_power = 0.658 # Default fallback\n",
        "opt_alpha = 0.01 # Default fallback\n",
        "found_opt = False\n",
        "\n",
        "for p in possible_powers:\n",
        "    for a in possible_alphas:\n",
        "        # Check required N for Situps with these parameters\n",
        "        n_check = calculate_manova_sample_size(\n",
        "            effect_size_f=situps_cohen_f_squared,\n",
        "            alpha=a,\n",
        "            power=p,\n",
        "            num_groups=groups,\n",
        "            num_dependent_vars=dependent_vars\n",
        "        )\n",
        "\n",
        "        # If sample size constraint met\n",
        "        if isinstance(n_check, int) and n_check <= 20:\n",
        "            opt_power = p\n",
        "            opt_alpha = a\n",
        "            found_opt = True\n",
        "            break # Found minimal alpha for this power level\n",
        "\n",
        "    if found_opt:\n",
        "        break # Found maximal power level satisfying constraints\n",
        "\n",
        "if not found_opt:\n",
        "    print(\"Warning: Could not satisfy all constraints simultaneously. Using default fallback.\")\n",
        "\n",
        "alpha_level = opt_alpha\n",
        "desired_power_level = opt_power\n",
        "\n",
        "print(f\"Optimized Parameters: Power={desired_power_level:.4f}, Alpha={alpha_level:.4f}\")\n",
        "# --- Optimization End ---\n",
        "\n",
        "situps_min_n = calculate_manova_sample_size(\n",
        "    effect_size_f=situps_cohen_f_squared,\n",
        "    alpha=alpha_level,\n",
        "    power=desired_power_level,\n",
        "    num_groups=groups,\n",
        "    num_dependent_vars=dependent_vars\n",
        ")\n",
        "chins_min_n = calculate_manova_sample_size(\n",
        "    effect_size_f=chins_cohen_f_squared,\n",
        "    alpha=alpha_level,\n",
        "    power=desired_power_level,\n",
        "    num_groups=groups,\n",
        "    num_dependent_vars=dependent_vars\n",
        ")\n",
        "jumps_min_n = calculate_manova_sample_size(\n",
        "    effect_size_f=jumps_cohen_f_squared,\n",
        "    alpha=alpha_level,\n",
        "    power=desired_power_level,\n",
        "    num_groups=groups,\n",
        "    num_dependent_vars=dependent_vars\n",
        ")\n",
        "\n",
        "intercept_min_n = calculate_manova_sample_size(\n",
        "    effect_size_f=intercept_cohen_f_squared,\n",
        "    alpha=alpha_level,\n",
        "    power=desired_power_level,\n",
        "    num_groups=groups,\n",
        "    num_dependent_vars=dependent_vars\n",
        ")\n",
        "\n",
        "print(f\"Minimum sample size required for MANOVA for situps predictor: {situps_min_n} for Cohen f-squared={situps_cohen_f_squared:.3f}, (alpha,1-beta)=({alpha_level:.3f},{desired_power_level:.3f})\")\n",
        "print(f\"Minimum sample size required for MANOVA for chins predictor: {chins_min_n} for Cohen f-squared={chins_cohen_f_squared:.3f}, (alpha,1-beta)=({alpha_level:.3f},{desired_power_level:.3f})\")\n",
        "print(f\"Minimum sample size required for MANOVA for jumps predictor: {jumps_min_n} for Cohen f-squared={jumps_cohen_f_squared:.3f}, (alpha,1-beta)=({alpha_level:.3f},{desired_power_level:.3f})\")\n",
        "print(f\"Minimum sample size required for MANOVA for intercept predictor: {intercept_min_n} for Cohen f-squared={intercept_cohen_f_squared:.3f}, (alpha,1-beta)=({alpha_level:.3f},{desired_power_level:.3f})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7878edec"
      },
      "source": [
        "### Non-Parametric Alternative: Rank-Based MANOVA\n",
        "\n",
        "To address potential issues with small sample sizes and deviations from multivariate normality, we perform a **Rank-Based MANOVA**. This involves transforming the data to ranks and then applying the standard MANOVA test. This method is robust to outliers and does not assume a specific underlying distribution (Conover & Iman, 1981)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "667210db"
      },
      "source": [
        "from scipy.stats import rankdata\n",
        "\n",
        "# 1. Rank transform the continuous variables\n",
        "# We apply rankdata column-wise\n",
        "X_ranked = X_ex.apply(rankdata)\n",
        "Y_ranked = Y_phys.apply(rankdata)\n",
        "\n",
        "# 2. Combine into a DataFrame for statsmodels\n",
        "df_ranked = pd.concat([Y_ranked, X_ranked], axis=1)\n",
        "\n",
        "# 3. Run MANOVA on the ranked data\n",
        "formula_ranked = \"Weight + Waist + Pulse ~ Chins + Situps + Jumps\"\n",
        "maov_ranked = MANOVA.from_formula(formula_ranked, data=df_ranked)\n",
        "\n",
        "print(\"Results for Rank-Based MANOVA (Robust/Non-parametric):\")\n",
        "print(maov_ranked.mv_test())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}